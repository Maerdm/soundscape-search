{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "This script gives insights about the dataset used for the web application. You can find the original dataset here:\n",
    "https://zenodo.org/records/7858848\n",
    "\n",
    "Since the audio files can not be published due to privacy reasons, we recommend to use the dataset.json shipped with this repository to try out the application. It is ready to use without the need of this script. If you want to use your own dataset, you can use this script as a guideline on how to sturcutre the dataset and on how to extract the acoustic features used in the aplication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is what a dataset should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# load dataset\n",
    "path = '../dataset.json'\n",
    "dataset = pd.read_json(path, dtype={'file_name': 'str'})\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if some of these Items are not in your dataset, you might want to create them and set them to nan, 0 or \"undefined\"\n",
    "\n",
    "- if items are missing, some functionalities in the webapp will not be working \n",
    "\n",
    "- all soundscape items are scaled from 0-4. The sliders in the web app are also sclaed from 0-4 (except the acoustic features)\n",
    "- --> The ranges of the acoustic features are the min and max value of each feature in the dataset. You have to set the slider ranges of the acoustic features manually in the webapp according to your dataset\n",
    "- --> set min max values in frontend/src/components/SliderRanges/sliderSoundscapeComponent.js\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get min max ranges of acoustic features\n",
    "# --> set min max values in frontend/src/components/SliderRanges/sliderSoundscapeComponent.js\n",
    "\n",
    "col = ['LAeq_default', 'N5_default', 'FavgArith_default', 'RAavgArith', 'SavgArith_default', 'R_default', 'T_default']\n",
    "\n",
    "min_max_values = dataset[col].agg({'min', 'max'})\n",
    "min_max_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset based on publication (zenodo repository, see link above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# load dataset\n",
    "path = '../dataset/02 Dataset.csv' # the original dataset from the zenodo repository\n",
    "df = pd.read_csv(path, sep=';')\n",
    "\n",
    "# create unique audio id\n",
    "dataset = df.copy()\n",
    "audio_id = dataset['ID'].astype(str) + '_' + dataset['Trigger_counter'].astype(str)\n",
    "dataset.insert(2, 'file_name', audio_id)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only specific columns\n",
    "to_keep = ['file_name', 'Soundscape_eventfulness', 'Soundscape_pleasantness', 'BGpleasant', 'BGchaotic', 'BGvibrant', 'BGuneventful', 'BGcalm', 'BGannoying', \n",
    "           'BGeventful', 'BGmonotonous', 'SC_Nature', 'SC_Human', 'SC_Household', 'SC_Installation', 'SC_Signals', 'SC_Traffic', 'SC_Speech', \n",
    "           'SC_Music', 'FGsource', 'Activity',  'Location8']\n",
    "\n",
    "dataset = dataset[to_keep]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "to_rename = {'Soundscape_eventfulness':'ISO_Eventfulness', 'Soundscape_pleasantness': 'ISO_Pleasantness', \n",
    "             'BGpleasant':'pleasant', 'BGchaotic':'chaotic', 'BGvibrant':'vibrant', 'BGuneventful':'uneventful', \n",
    "             'BGcalm':'calm', 'BGannoying':'annoying', 'BGeventful':'eventful', 'BGmonotonous':'monotonous'}\n",
    "\n",
    "dataset.rename(columns=to_rename, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add duration in seconds and suffix:\n",
    "dataset.insert(1, 'duration_s', 15)\n",
    "dataset.insert(2, 'suffix', '.wav')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new value ranges of soundscape items\n",
    "def range_zero_to_four(x):\n",
    "    return (x / (4 + np.sqrt(32)) + 1) * 2\n",
    "\n",
    "def sc_range(x):\n",
    "    x = round(x * 0.4, 1)\n",
    "\n",
    "    return x.astype(float)\n",
    "\n",
    "dataset['ISO_Eventfulness'] = dataset['ISO_Eventfulness'].apply(range_zero_to_four)\n",
    "dataset['ISO_Pleasantness'] = dataset['ISO_Pleasantness'].apply(range_zero_to_four)\n",
    "\n",
    "col = ['SC_Nature', 'SC_Human', 'SC_Household', 'SC_Installation', 'SC_Signals', 'SC_Traffic', 'SC_Speech', 'SC_Music']\n",
    "dataset[col] = dataset[col].apply(sc_range)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADd acoustic features from AcousticFeatures_SingleValues.csv dataset\n",
    "path = '../dataset/AcousticFeatures_SingleValues.csv'\n",
    "acoustic_dataset = pd.read_csv(path, sep=';')\n",
    "\n",
    "# items to keep from acoustic dataset\n",
    "columns_to_select = ['Key', 'Channel', 'LAeq_default', 'N5_default', 'FavgArith_default', 'RAavgArith', 'SavgArith_default', 'R_default', 'T_default']\n",
    "acoustic_dataset = acoustic_dataset[columns_to_select]\n",
    "\n",
    "acoustic_dataset_max_values = acoustic_dataset.groupby('Key').max().reset_index()\n",
    "acoustic_dataset_max_values = acoustic_dataset_max_values.drop(columns=['Channel'])\n",
    "acoustic_dataset_max_values.head()\n",
    "\n",
    "# calculate mean of both audio channels\n",
    "#acoustic_dataset = acoustic_dataset.groupby('Key').mean().reset_index()\n",
    "# acoustic_dataset = acoustic_dataset.drop(columns=['Channel'])\n",
    "# acoustic_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataset and acoustic_dataset\n",
    "final_dataset = dataset.merge(acoustic_dataset_max_values, left_on='file_name', right_on='Key', how='left')\n",
    "final_dataset = final_dataset.drop(columns=['Key'])\n",
    "final_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get min max ranges of acoustic features\n",
    "# --> used for Sliders in WebApp\n",
    "\n",
    "col = ['LAeq_default', 'N5_default', 'FavgArith_default', 'RAavgArith', 'SavgArith_default', 'R_default', 'T_default']\n",
    "\n",
    "min_max_values = final_dataset[col].agg({'min', 'max'})\n",
    "min_max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store dataset\n",
    "final_dataset.to_csv('../dataset/FinalDataset.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add time-variant acoustic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mosqito.sq_metrics import sharpness_din_tv, loudness_zwtv, roughness_dw, pr_ecma_perseg, tnr_ecma_perseg\n",
    "import numpy as np\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute calibration factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_REF = 20e-6 \n",
    "\n",
    "def sound_pressure(audio_data):\n",
    "    p_rms = np.sqrt(np.mean(audio_data**2))  # RMS berechnen\n",
    "    laeq = 20 * np.log10(p_rms / P_REF)  # Umrechnung in dB SPL\n",
    "    return laeq\n",
    "\n",
    "def calibration_value(audio_data, expected_l_ch1, expected_l_ch2):\n",
    "    raw_laeg_ch1 = sound_pressure(audio_data[:, 0])\n",
    "    raw_laeg_ch2 = sound_pressure(audio_data[:, 1])\n",
    "\n",
    "    cal_ch1 = expected_l_ch1 - raw_laeg_ch1\n",
    "    cal_ch2 = expected_l_ch2 - raw_laeg_ch2\n",
    "\n",
    "    return cal_ch1, cal_ch2\n",
    "\n",
    "def apply_calibration(audio_data, cal_ch1, cal_ch2):\n",
    "    audio_calibrated = np.zeros_like(audio_data)\n",
    "    audio_calibrated[:, 0] = audio_data[:, 0] * (10**(cal_ch1 / 20))  # dB-Wert in linear gain factor\n",
    "    audio_calibrated[:, 1] = audio_data[:, 1] * (10**(cal_ch2 / 20)) \n",
    "    return audio_calibrated\n",
    "\n",
    "# load audio\n",
    "file_path = \"../dataset/audiofiles/1132730_16.wav\"\n",
    "audio_data, fs = sf.read(file_path)\n",
    "\n",
    "# callibrate\n",
    "cal_ch1, cal_ch2 = calibration_value(audio_data, 64.13, 64.57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute acoustic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute audio features and save to dataset\n",
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "def compute_acoustic_features(audio, sr):\n",
    "\n",
    "    N_time, _, _, _ = loudness_zwtv(audio, sr) # N_time\n",
    "    S, _ = sharpness_din_tv(audio, fs=sr) # S\n",
    "    R_time, _, _, _ = roughness_dw(audio, sr) # R_time\n",
    "    t_pr, _, _, _, _ = pr_ecma_perseg(audio, sr) # t_pr\n",
    "    t_tnr, _, _, _, _ = tnr_ecma_perseg(audio, sr) # t_tnr\n",
    "        \n",
    "    return N_time, S, R_time, t_pr, t_tnr\n",
    "\n",
    "def scale_acoustic_features_to_audio_length(feature_array, audio_length):\n",
    "\n",
    "\n",
    "    original_length = len(feature_array)\n",
    "    block_size = original_length / audio_length\n",
    "    result = np.zeros(audio_length)\n",
    "    \n",
    "    for i in range(audio_length):\n",
    "        start_idx = int(i * block_size)\n",
    "        end_idx = int(min((i + 1) * block_size, original_length))\n",
    "        \n",
    "        # Average the values in this block\n",
    "        if start_idx < end_idx:\n",
    "            result[i] = np.mean(feature_array[start_idx:end_idx])\n",
    "        else:\n",
    "            result[i] = feature_array[start_idx] if start_idx < original_length else 0\n",
    "            \n",
    "    return result\n",
    "\n",
    "folder_path = '/dataset/audiofiles'\n",
    "audio_files = [f for f in os.listdir(folder_path) if f.endswith(('.wav'))]\n",
    "    \n",
    "    # Process each audio file\n",
    "for audio_file in audio_files:\n",
    "    file_path = os.path.join(folder_path, audio_file)\n",
    "    \n",
    "    try:\n",
    "        # Load audio\n",
    "        audio_data, sr = sf.read(file_path)\n",
    "\n",
    "        # calibrate audio and convert to mono\n",
    "        calibrated_audio = apply_calibration(audio_data, cal_ch1, cal_ch2)\n",
    "        mono_audio = np.mean(calibrated_audio, axis=1)\n",
    "\n",
    "        # compute acoustic features\n",
    "        loudness, sharpness, roughness, prominence_ratio, tone_to_noise_ratio = compute_acoustic_features(mono_audio, sr)\n",
    "\n",
    "        # averagte features to audio length\n",
    "        time_scaling_factor = 1\n",
    "        length_audio = np.floor(len(mono_audio)/sr) + 1\n",
    "        loudness = scale_acoustic_features_to_audio_length(loudness, int((length_audio*time_scaling_factor)))\n",
    "        sharpness = scale_acoustic_features_to_audio_length(sharpness, int((length_audio*time_scaling_factor)))\n",
    "        roughness = scale_acoustic_features_to_audio_length(roughness, int((length_audio*time_scaling_factor)))\n",
    "\n",
    "        audio_features = {\n",
    "            \"loudness\": loudness.tolist(),\n",
    "            \"sharpness\": sharpness.tolist(),\n",
    "            \"roughness\": roughness.tolist(),\n",
    "            \"prominence_ratio\": prominence_ratio.tolist(),\n",
    "            \"tone_to_noise_ratio\": tone_to_noise_ratio.tolist()\n",
    "        }\n",
    "\n",
    "        # push to dataset\n",
    "        file_name = file_path.split('/')[-1].split('.')[0]\n",
    "        row_idx = df[df['file_name'] == file_name].index.to_list()\n",
    "\n",
    "        if row_idx:\n",
    "\n",
    "            # store to your dataset\n",
    "            df.at[row_idx[0], 'temporal_audio_features'] = [audio_features]\n",
    "            print('\\n done processing file ', audio_file)\n",
    "            print('\\n')\n",
    "        else:\n",
    "            print(f\"No row found with file_name: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_file}: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
